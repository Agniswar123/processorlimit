{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ensembled feature selection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Agniswar123/processorlimit/blob/master/ensembled_feature_selection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdAO5oevSNxr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "30adf716-a16e-4583-edef-9178d991b8ce"
      },
      "source": [
        "!pip install skfeature-chappers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting skfeature-chappers\n",
            "  Downloading https://files.pythonhosted.org/packages/e6/45/19bb801eb3b4a892534ab86468ad0669a68ff63578610f90051190e3622f/skfeature-chappers-1.0.3.tar.gz\n",
            "Building wheels for collected packages: skfeature-chappers\n",
            "  Building wheel for skfeature-chappers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for skfeature-chappers: filename=skfeature_chappers-1.0.3-py2.py3-none-any.whl size=59512 sha256=30e28e6c31ffa3899335ed114e7e4861190a357fb81a44eb07d8f302680cc03d\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/61/bf/1b3a8c232a0072409508c2ec4c12f316e95681ae72ba7315d2\n",
            "Successfully built skfeature-chappers\n",
            "Installing collected packages: skfeature-chappers\n",
            "Successfully installed skfeature-chappers-1.0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVL-pw8Muryu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "#from sklearn.tree import DecisionTreeClassifier   Not used here\n",
        "from sklearn.neighbors import KNeighborsClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1xhUpvJFKdG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "b7f7f2c6-83ee-453b-cbb5-3503a2f5c4d9"
      },
      "source": [
        "#read data\n",
        "path=\"colon.txt\"\n",
        "data=pd.read_csv(path,delimiter=\"\\t\" , index_col=None, header=None)\n",
        "print(data.head())\n",
        "print(data.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   0     1     2     3     ...        1997       1998       1999  2000\n",
            "0  8590  5470  4260  4060  ...   75.699997  83.500000  28.700001     1\n",
            "1  3830  6970  5370  4710  ...   42.700001  16.100000  15.200000     1\n",
            "2  3230  3690  3400  3460  ...   57.599998   7.490000  31.799999     1\n",
            "3  3230  3690  3400  3460  ...   57.599998   7.490000  31.799999     1\n",
            "4  9330  7020  4720  9490  ...  122.000000  40.400002  26.799999     1\n",
            "\n",
            "[5 rows x 2001 columns]\n",
            "(62, 2001)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyO3N7j0F7xe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b46c83bd-cd79-4a22-b593-b8f0b08557f2"
      },
      "source": [
        "#split in input and output\n",
        "x=data.iloc[:,:-1]\n",
        "y=data.iloc[:,-1]\n",
        "samples=(data.shape[0]) \n",
        "title=path.split(\".\")[0]\n",
        "\n",
        "print(title)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "colon\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDdhlCoqVT2g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "f8f102ae-8aff-4501-cce0-996e2a90528a"
      },
      "source": [
        "#gridsearchcv to evaluate parameters of different classifiers\n",
        "#this nees to be done before apllying filter or any further data split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "#SVM\n",
        "parameters = {'kernel':('linear', 'poly', 'rbf', 'sigmoid'), 'C':[i for i in range(1,10,1)]}\n",
        "svc = SVC()\n",
        "\n",
        "svc_best = GridSearchCV(svc, parameters)\n",
        "svc_best.fit(X=x, y=y)\n",
        "print(svc_best.best_estimator_)\n",
        "\n",
        "#NB dont take any parameters so its not checked\n",
        "#KNN\n",
        "parameters = {'weights':('uniform', 'distance'), 'algorithm':('auto','brute','ball_tree','kd_tree'), 'metric':('euclidean','manhattan','chebyshev'), 'n_neighbors':[i for i in range(3,15,2)], 'leaf_size':[j for j in range(30,150,5)], 'p':[k for k in range(1,5,1)]}\n",
        "knn= KNeighborsClassifier()\n",
        "knn_best=GridSearchCV(knn,parameters)\n",
        "knn_best.fit(X=x, y=y)\n",
        "print(knn_best.best_estimator_)\n",
        "\n",
        "\"\"\"\n",
        "This function took almost 8-10 mins to execute. thats why the outputs are stored separately and no variable is used\n",
        "SVC(C=6, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
        "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
        "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
        "    tol=0.001, verbose=False)\n",
        "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='manhattan',\n",
        "                     metric_params=None, n_jobs=None, n_neighbors=3, p=1,\n",
        "                     weights='uniform')\n",
        "\"\"\"\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVC(C=6, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
            "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
            "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
            "    tol=0.001, verbose=False)\n",
            "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='manhattan',\n",
            "                     metric_params=None, n_jobs=None, n_neighbors=3, p=1,\n",
            "                     weights='uniform')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3O_53unORowr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#fisher score function\n",
        "from scipy.sparse import *\n",
        "def fisher_score(X, y):\n",
        "    import numpy as np\n",
        "    \n",
        "    from skfeature.utility.construct_W import construct_W\n",
        "    \"\"\"\n",
        "    This function implements the fisher score feature selection, steps are as follows:\n",
        "    1. Construct the affinity matrix W in fisher score way\n",
        "    2. For the r-th feature, we define fr = X(:,r), D = diag(W*ones), ones = [1,...,1]', L = D - W\n",
        "    3. Let fr_hat = fr - (fr'*D*ones)*ones/(ones'*D*ones)\n",
        "    4. Fisher score for the r-th feature is score = (fr_hat'*D*fr_hat)/(fr_hat'*L*fr_hat)-1\n",
        "\n",
        "    Input\n",
        "    -----\n",
        "    X: {numpy array}, shape (n_samples, n_features)\n",
        "        input data\n",
        "    y: {numpy array}, shape (n_samples,)\n",
        "        input class labels\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "    score: {numpy array}, shape (n_features,)\n",
        "        fisher score for each feature\n",
        "\n",
        "    Reference\n",
        "    ---------\n",
        "    He, Xiaofei et al. \"Laplacian Score for Feature Selection.\" NIPS 2005.\n",
        "    Duda, Richard et al. \"Pattern classification.\" John Wiley & Sons, 2012.\n",
        "    \"\"\"\n",
        "\n",
        "    # Construct weight matrix W in a fisherScore way\n",
        "    kwargs = {\"neighbor_mode\": \"supervised\", \"fisher_score\": True, 'y': y}\n",
        "    W = construct_W(X, **kwargs)\n",
        "\n",
        "    # build the diagonal D matrix from affinity matrix W\n",
        "    D = np.array(W.sum(axis=1))\n",
        "    L = W\n",
        "    tmp = np.dot(np.transpose(D), X)\n",
        "    D = diags(np.transpose(D), [0])\n",
        "    Xt = np.transpose(X)\n",
        "    t1 = np.transpose(np.dot(Xt, D.todense()))\n",
        "    t2 = np.transpose(np.dot(Xt, L.todense()))\n",
        "    # compute the numerator of Lr\n",
        "    D_prime = np.sum(np.multiply(t1, X), 0) - np.multiply(tmp, tmp)/D.sum()\n",
        "    # compute the denominator of Lr\n",
        "    L_prime = np.sum(np.multiply(t2, X), 0) - np.multiply(tmp, tmp)/D.sum()\n",
        "    # avoid the denominator of Lr to be 0\n",
        "    D_prime[D_prime < 1e-12] = 10000\n",
        "    lap_score = 1 - np.array(np.multiply(L_prime, 1/D_prime))[0, :]\n",
        "\n",
        "    # compute fisher score from laplacian score, where fisher_score = 1/lap_score - 1\n",
        "    score = 1.0/lap_score - 1\n",
        "    return np.transpose(score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "451oMSOfTMVr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#signal to noise ratio\n",
        "#using weighted one-vs-all strategy for multi-class data\n",
        "def signaltonoise(feature, target, axis = 0, ddof = 0):\n",
        "  import numpy as np\n",
        "  classes = np.unique(target)\n",
        "  if len(feature.shape)<2:\n",
        "    feature = feature.reshape(-1,1)\n",
        "  row, _ = feature.shape\n",
        "  if len(classes) <= 2:\n",
        "    m = None\n",
        "    std = 0\n",
        "    for each in classes:\n",
        "      idx = np.where(target == each)[0]\n",
        "      #convinient way of doing m1-m2\n",
        "      if m is None:\n",
        "        m = feature.iloc[idx, :].mean(axis)\n",
        "      else:\n",
        "        m -= feature.iloc[idx, :].mean(axis)\n",
        "\n",
        "      #sd1+sd2\n",
        "      std += feature.iloc[idx, :].std(axis = axis, ddof = ddof)\n",
        "\n",
        "    return np.asanyarray(m/std)\n",
        "\n",
        "  else:\n",
        "    snr_scores = [] #for storing the weighted scores\n",
        "    #using the one vs all strategy for each class with\n",
        "    for each in classes:\n",
        "      idx = np.where(target == each)[0]\n",
        "      idxn = np.where(target != each)[0]\n",
        "      m = feature.iloc[idx, :].mean(axis) - feature.iloc[idxn, :].mean(axis)\n",
        "      std = feature.iloc[idx, :].std(axis = axis, ddof = ddof) + feature.iloc[idxn, :].std(axis = axis, ddof = ddof) \n",
        "      snr_scores.append((m/std) * len(idx)/row) #weighted snr\n",
        "\n",
        "    return np.asanyarray(snr_scores).sum(axis = axis)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SKQWk1-SoD9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#this function is needed during polling, \n",
        "#if else cannot handle dataset more than 2 class labels\n",
        "def mostFrequent(arr): \n",
        "  \n",
        "    # Sort the array \n",
        "    arr.sort() \n",
        "    n=len(arr)\n",
        "    # find the max frequency using linear traversal \n",
        "    max_count = 1; res = arr[0]; curr_count = 1\n",
        "      \n",
        "    for i in range(1, n):  \n",
        "        if (arr[i] == arr[i - 1]): \n",
        "            curr_count += 1\n",
        "              \n",
        "        else : \n",
        "            if (curr_count > max_count):  \n",
        "                max_count = curr_count \n",
        "                res = arr[i - 1] \n",
        "              \n",
        "            curr_count = 1\n",
        "      \n",
        "    # If last element is most frequent \n",
        "    if (curr_count > max_count): \n",
        "      \n",
        "        max_count = curr_count \n",
        "        res = arr[n - 1] \n",
        "      \n",
        "    return int(res) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nrBZg2oWQMG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def feature_ranking(score):\n",
        "    \"\"\"\n",
        "    Rank features in descending order according to fisher score, the larger the fisher score, the more important the\n",
        "    feature is\n",
        "    \"\"\"\n",
        "    idx = np.argsort(score, 0)\n",
        "    return idx[::-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZPSNuRQWP29",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "19ade7f0-2557-4856-bb32-8ad52939f0d5"
      },
      "source": [
        "#finding Mutual Information score of data.\n",
        "MI=mutual_info_classif(x,y)\n",
        "\n",
        "#finding Signal to Noise ratio score of data.\n",
        "SNR=signaltonoise(x,y)\n",
        "\n",
        "#finding Fisher Score score of data.\n",
        "FS=fisher_score(x.values,y)\n",
        "\n",
        "#selecting best n features.\n",
        "n=5\n",
        "\n",
        "#after sorting the score array, first n gene indices are taken\n",
        "MI_indices=feature_ranking(MI)[:n]\n",
        "SNR_indices=feature_ranking(SNR)[:n]\n",
        "FS_indices=feature_ranking(FS)[:n]\n",
        "\n",
        "#result\n",
        "print(MI_indices,\"\\n\",SNR_indices,\"\\n\",FS_indices)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 624 1866 1670  512 1934] \n",
            " [ 248  764  492 1422  244] \n",
            " [ 248  764  492 1422  244]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjndOh-jWDeR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create subset feature of 3 scoring functions\n",
        "#y remains same for all, only x changes\n",
        "\n",
        "x_mi=[]\n",
        "x_snr=[]\n",
        "x_fs=[]\n",
        "\n",
        "#appending the columns to a list\n",
        "for i in range(n):  #from previous cell, n is no of selected genes\n",
        "  x_mi.append(x.iloc[:,MI_indices[i]])\n",
        "  x_snr.append(x.iloc[:,SNR_indices[i]])\n",
        "  x_fs.append(x.iloc[:,FS_indices[i]])\n",
        "\n",
        "#converting list to dataframe and then transposing it as the data was added rowwise bt we need columnwise\n",
        "x_mi=pd.DataFrame(x_mi)\n",
        "x_mi=x_mi.transpose()\n",
        "x_snr=pd.DataFrame(x_snr)\n",
        "x_snr=x_snr.transpose()\n",
        "x_fs=pd.DataFrame(x_fs)\n",
        "x_fs=x_fs.transpose()\n",
        "\n",
        "#print(x_mi,x_snr,x_fs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Frc8vjANanvT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#objects of used classifiers\n",
        "var=int(samples**0.5)\n",
        "if var%2==0:\n",
        "  var=var+1\n",
        "#use odd no of neighbours for voting and taking sqrt(n) neighbours\n",
        "KNN_mi = KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='manhattan',\n",
        "                     metric_params=None, n_jobs=None, n_neighbors=3, p=1,\n",
        "                     weights='uniform')\n",
        "KNN_snr = KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='manhattan',\n",
        "                     metric_params=None, n_jobs=None, n_neighbors=3, p=1,\n",
        "                     weights='uniform')\n",
        "KNN_fs = KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='manhattan',\n",
        "                     metric_params=None, n_jobs=None, n_neighbors=3, p=1,\n",
        "                     weights='uniform') \n",
        "SVM_mi = SVC(C=6, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
        "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
        "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
        "    tol=0.001, verbose=False)\n",
        "SVM_snr = SVC(C=6, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
        "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
        "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
        "    tol=0.001, verbose=False)\n",
        "SVM_fs = SVC(C=6, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
        "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
        "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
        "    tol=0.001, verbose=False)\n",
        "NB_mi = GaussianNB()\n",
        "NB_snr = GaussianNB()\n",
        "NB_fs = GaussianNB()\n",
        "#Tree = DecisionTreeClassifier()\n",
        "no_filter,no_classifier=3,3\n",
        "#the ensembled classifier based on results of these classifiers, so these results are all we need"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpOG2maiGmkl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "cdd166d6-768e-44ec-9107-5395fecb195c"
      },
      "source": [
        "loo=LeaveOneOut()\n",
        "\n",
        "ensemble=[]\n",
        "score=0\n",
        "#it can be further optimized as y is not modified\n",
        "fname=title+str(n)+\"_genes.txt\"\n",
        "f=open(fname,\"w\")\n",
        "f.write(\"KNN\\tSVM\\tNB\\tEnsemble\\tActual\\n\")\n",
        "\n",
        "for train_index, test_index in loo.split(x_mi):\n",
        "  \n",
        "  x_mi_train_data,y_mi_train=x_mi.iloc[train_index,:],y[train_index]\n",
        "  x_mi_test_data,y_mi_test=x_mi.iloc[test_index,:],y[test_index]\n",
        "  KNN_mi.fit(x_mi_train_data,y_mi_train)\n",
        "  SVM_mi.fit(x_mi_train_data,y_mi_train)\n",
        "  NB_mi.fit(x_mi_train_data,y_mi_train)\n",
        "  \n",
        "  x_snr_train_data,y_snr_train=x_snr.iloc[train_index,:],y[train_index]\n",
        "  x_snr_test_data,y_snr_test=x_snr.iloc[test_index,:],y[test_index]\n",
        "  KNN_snr.fit(x_snr_train_data,y_snr_train)\n",
        "  SVM_snr.fit(x_snr_train_data,y_snr_train)\n",
        "  NB_snr.fit(x_snr_train_data,y_snr_train)\n",
        "\n",
        "  x_fs_train_data,y_fs_train=x_fs.iloc[train_index,:],y[train_index]\n",
        "  x_fs_test_data,y_fs_test=x_fs.iloc[test_index,:],y[test_index]\n",
        "  KNN_fs.fit(x_fs_train_data,y_fs_train)\n",
        "  SVM_fs.fit(x_fs_train_data,y_fs_train)\n",
        "  NB_fs.fit(x_fs_train_data,y_fs_train)\n",
        "\n",
        "  \n",
        "  classifier_mi=[KNN_mi,SVM_mi,NB_mi]\n",
        "  ind_res_mi=[]\n",
        "\n",
        "  classifier_snr=[KNN_snr,SVM_snr,NB_snr]\n",
        "  ind_res_snr=[]\n",
        "\n",
        "  classifier_fs=[KNN_fs,SVM_fs,NB_fs]\n",
        "  ind_res_fs=[]\n",
        "\n",
        "  ind_res=np.zeros((no_filter,no_classifier))   #3 filter, 3 classifiers\n",
        "  #here test data is tested against 3 classifiers, no cross set test data has been used\n",
        "  for i in range(len(classifier_mi)):\n",
        "    tmp=[]\n",
        "    y_mi_predict=classifier_mi[i].predict(x_mi_test_data)\n",
        "    y_snr_predict=classifier_snr[i].predict(x_snr_test_data)\n",
        "    y_fs_predict=classifier_fs[i].predict(x_fs_test_data)\n",
        "    #data getting stored as KNN: midata, snrdata, fsdata\n",
        "    ind_res[i][0]=y_mi_predict\n",
        "    ind_res[i][1]=y_snr_predict\n",
        "    ind_res[i][2]=y_fs_predict\n",
        "\n",
        "  # checking KNN: MI,SNR,FS and taking combined result over 3 types of sub-dataset\n",
        "  classifier_indiv=[]\n",
        "  #this counting is the ensemble part. the more classifiers support a decision, that one is taken \n",
        "  for i in range(no_classifier): #this is no of classifiers\n",
        "    tmp=ind_res[i].tolist()\n",
        "    var=mostFrequent(tmp)\n",
        "    classifier_indiv.append(var)\n",
        "    f.write(\"%d\\t\\t\"%var)\n",
        "\n",
        "  # taking result of 3 classifiers and taking their combined result\n",
        "  var=mostFrequent(classifier_indiv)\n",
        "  ensemble.append(var)\n",
        "  f.write(\"%d\\t\\t%d\\n\"%(var,y_mi_test.iloc[0]))\n",
        "  if var==y_mi_test.iloc[0]:\n",
        "    score+=1\n",
        "\n",
        "  #write in a file\n",
        "\n",
        " \n",
        "f.write(\"\\n\\nScore: %d\\tTotal: %d\\nAccuracy: %f\"%(score,samples,score/samples))\n",
        "f.close()\n",
        "print(ensemble,\"\\nScore: \",score,\"\\nTotal: \",samples,\"\\nAccuracy: \",score/samples)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0] \n",
            "Score:  52 \n",
            "Total:  62 \n",
            "Accuracy:  0.8387096774193549\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}